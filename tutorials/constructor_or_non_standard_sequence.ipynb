{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructor or non standard sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In previous tutorials it was discussed how to perform calculations with standard NICE scheme, which is reflected by class StandardSequence. But NICE toolbox provides broader opportunities. It is possible, for example, to combine latest covariants with each other at each step in order to get 2^n body order features after n iterations. \n",
    "\n",
    "In previous tutorials model was defined by StandardSequence class, whose initialization method accepts instances of other classes as ThresholdExpansioner or InvariantsPurifier. These blocks can be used by their own to construct custom model.\n",
    "\n",
    "First of all we need to calculate spherical expansion coefficients as in previous tutorials:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downloading dataset from https://archive.materialscloud.org/record/2020.110\n",
    "\n",
    "!wget \"https://archive.materialscloud.org/record/file?file_id=b612d8e3-58af-4374-96ba-b3551ac5d2f4&filename=methane.extxyz.gz&record_id=528\" -O methane.extxyz.gz\n",
    "!gunzip -k methane.extxyz.gz\n",
    "\n",
    "import numpy as np\n",
    "import ase.io\n",
    "import tqdm\n",
    "from nice.blocks import *\n",
    "from nice.utilities import *\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "\n",
    "structures = ase.io.read('methane.extxyz', index='0:1000')\n",
    "\n",
    "HYPERS = {\n",
    "    'interaction_cutoff': 6.3,\n",
    "    'max_radial': 5,\n",
    "    'max_angular': 5,\n",
    "    'gaussian_sigma_type': 'Constant',\n",
    "    'gaussian_sigma_constant': 0.05,\n",
    "    'cutoff_smooth_width': 0.3,\n",
    "    'radial_basis': 'GTO'\n",
    "}\n",
    "\n",
    "all_species = get_all_species(structures)\n",
    "\n",
    "coefficients = get_spherical_expansion(structures, HYPERS, all_species)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "up to this point coefficients for each central specie are 4 dimensional numpy array with indexing [environmental index, radial basis/specie index, lambda, m]\n",
    "\n",
    "Let's focus on only H centered environments:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients = coefficients[1]\n",
    "print(coefficients.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The first step is to perform initial scaling, as it was discussed in the first tutorial. For this purposes there is class InitialScaler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_scaler = InitialScaler(mode='signal integral', individually=False)\n",
    "initial_scaler.fit(coefficients)\n",
    "coefficients = initial_scaler.transform(coefficients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If individually is set to False this class requires fitting before transforming the data. Otherwise fitting is not required. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are going to track parity of covariants, i. e. keep even and odd features separated, we need to split them at the begining of our calculations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_even_1, data_odd_1 = InitialTransformer().transform(coefficients)\n",
    "print(type(data_even_1))\n",
    "print(data_even_1.covariants_.shape)\n",
    "print(\"even features sizes: \", data_even_1.actual_sizes_)\n",
    "print(\"odd features sizes: \", data_odd_1.actual_sizes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is couple of Data instances which was already discussed in the tutorial \"Calculating covariants\".\n",
    "\n",
    "All spherical expansion coefficients with even l remain constant under reflections, i. e. are even covariants, while all spherical expansion coefficients with odd l changes sign under reflection, i. e. are odd covariants. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA and purifiers blocks has two versions. One to transform single instance of data of certain parity, and the second is for the same transformation of both. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = IndividualLambdaPCAs(n_components=5)  #single parity version\n",
    "pca.fit(data_even_1)\n",
    "data_even_1_t = pca.transform(data_even_1)\n",
    "print(data_even_1_t.actual_sizes_)\n",
    "\n",
    "pca = IndividualLambdaPCAsBoth()  #both version\n",
    "pca.fit(data_even_1, data_odd_1)\n",
    "data_even_1_t, data_odd_1_t = pca.transform(data_even_1, data_odd_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One common thing among PCA and purifiers blocks is num_to_fit semantics. Each class has num_to_fit argument in the initialization, which by default equals to '10x'. If num_to_fit is string of 'number x' format it would cause corresponding class use no more than number multiplier by number of components in case of pca, or number multiplier by number of coefficients in linear regression in case of purifiers data points. Data points are calculated as all entries of covariants. I. e. for lambda = 3 for example each environment would bring (3 * 2 + 1) data points, since dimensionality of single covariant vector is (2 * lambda + 1). If num_to_fit is int, it would do the same using the provided number as the upper bound for number of datapoints not depending on the actual number of pca components or linear regression coefficients. If total available number of data points is less than the number specified by num_to_fit class would raise warning, that there are not enough data. If num_to_fit is None corresponding block would always use all available data for fitting.\n",
    "\n",
    "This is done because the overall model is very diverse, and different parts of the model requires very different amount of data for good fitting. Thus, it is a good idea to do such restrictions to speed up the process. \n",
    "\n",
    "In case of PCA if n_components specified in the constructor is less than the actual number of features given during the fit step, it would be decreased to actual number of features.\n",
    "But, if number of data points is less than number of components after this possible decreasement (which make it impossible to produce such amount of components) it would raise ValueError with demand to provide more data for fitting. \n",
    "\n",
    "In order to do PCA step in invariants branch there is class InvariantsPCA, which actually differs from sklearn.decomposition.PCA only by num_to_fit semantics: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = InvariantsPCA(num_to_fit='300x')\n",
    "ar = np.random.rand(400, 10)\n",
    "pca.fit(ar)\n",
    "print(pca.transform(ar).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For purifiers there are classes CovariantsPurifier, CovariantsPurifierBoth, \n",
    "InvariantsPurifier, and CovariantsIndividualPurifier. Their purpose is to transform data of single parity, both chunks of data, invariants, and single lambda channel respectively.\n",
    "\n",
    "Their fit and transform methods accept list of covariants/invariants of previous body orders along with current body order. For example: (Let's pretend that we have already features of several body orders):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "purifier = CovariantsPurifier(max_take=3)\n",
    "purifier.fit([data_even_1, data_even_1], data_even_1)\n",
    "data_even_1_t = purifier.transform([data_even_1, data_even_1], data_even_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it was already mentioned in the first tutorial purifiers can accept arbitrarily sklearn shaped linear regressors, i. e. with fit and predict methods. See tutorial \"Custom regressors into purifiers\" for example of such custom regressor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to do expansion with thresholding euristics it is necessary to get information how important are particular features. One way is to assing .importance_ property in the Data class (setter will be done in the next version of NICE). The other is to pass features through pca, which would automatically asign importances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = IndividualLambdaPCAsBoth()\n",
    "pca.fit(data_even_1, data_odd_1)\n",
    "data_even_1, data_odd_1 = pca.transform(data_even_1, data_odd_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ThresholdExpansioner's fit and transform methods accept two even-odd pair of datas. If first pair is of body order v1 and second pair is of body order v2, result would be of body order v1 + v2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expansioner = ThresholdExpansioner(num_expand=200)\n",
    "\n",
    "expansioner.fit(data_even_1, data_odd_1, data_even_1, data_odd_1)\n",
    "data_even_2, data_odd_2 = expansioner.transform(data_even_1, data_odd_1,\\\n",
    "                                                data_even_1, data_odd_1)\n",
    "print(data_even_2.actual_sizes_)\n",
    "print(data_odd_2.actual_sizes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most time during the fitting is consumed for precomputing clebsch-gordan coefficients. Thus, in case of frequent expansioners fitting with same lambda_max, it is a good idea to precompute clebsch-gordan coefficients once, and after that just feed expansioners with them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clebsch = nice.clebsch_gordan.ClebschGordan(5)  # 5 is lamba max\n",
    "\n",
    "expansioner = ThresholdExpansioner(num_expand=200)\n",
    "expansioner.fit(data_even_1,\n",
    "                data_odd_1,\n",
    "                data_even_1,\n",
    "                data_odd_1,\n",
    "                clebsch_gordan=clebsch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It might be usefull to investigate how actually usefull is thresholding heuristic in practice. For this purpose it is possible to get \"raw importances\" for output features which are multiplication of importances of input features which were used in Clebsch-Gordan iteration. In other words it is the criterion for selection itself. \n",
    "\n",
    "Let's plot scatter plot which would show how selection criterion correlates with variance of output features for example. We will use invariants for simplicity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expansioner = ThresholdExpansioner(num_expand=200, mode='invariants')\n",
    "expansioner.fit(data_even_1,\n",
    "                data_odd_1,\n",
    "                data_even_1,\n",
    "                data_odd_1,\n",
    "                clebsch_gordan=clebsch)\n",
    "invariants_even, _ = expansioner.transform(data_even_1, data_odd_1,\\\n",
    "                                                data_even_1, data_odd_1)\n",
    "\n",
    "print(invariants_even.shape)\n",
    "\n",
    "variances = np.mean(((invariants_even - np.mean(invariants_even, axis=0))**2),\n",
    "                    axis=0)\n",
    "raw_importances = expansioner.new_even_raw_importances_\n",
    "\n",
    "plt.plot(np.sqrt(raw_importances), variances, 'o')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('raw importance')\n",
    "plt.ylabel('variance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is some correlation. Thus, tresholding heuristic works. Getters for raw importances might be inserted in next version of NICE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard block has the same input to fit and transform methods as TresholdExpansioner if it doesn't contain purifers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block = StandardBlock(ThresholdExpansioner(num_expand=200), None,\n",
    "                      IndividualLambdaPCAsBoth(n_components=10))\n",
    "block.fit(data_even_1, data_odd_1, data_even_1, data_odd_1)\n",
    "data_even_2, data_odd_2, invariants_even = block.transform(data_even_1, data_odd_1,\\\n",
    "                                                data_even_1, data_odd_1)\n",
    "print(data_even_2.actual_sizes_)\n",
    "print(invariants_even)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case invariants branch was None, and thus it returned None for invariants. This behavior is opposite to StandardSequence one, since it always returns invariants. If invariants branch of some block would be None it would return [:, :, 0, 0] part of covariants. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If block contains invariants purifier, than old_even_invariants should be specified in fit and transform methods. If block contains covariants purifier, than old_even_covariants and old_odd_covariants should be specified. \n",
    "\n",
    "old_even_invariants should be list of 2 dimensional numpy arrays with previous invariants, old_even_covariants and old_odd_covariants should be lists with Data instances. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block = StandardBlock(ThresholdExpansioner(num_expand=200),\n",
    "                      CovariantsPurifierBoth(max_take=10), None,\n",
    "                      ThresholdExpansioner(num_expand=200, mode='invariants'),\n",
    "                      InvariantsPurifier(max_take=10), None)\n",
    "\n",
    "block.fit(\n",
    "    data_even_2,\n",
    "    data_odd_2,\n",
    "    data_even_1,\n",
    "    data_odd_1,\n",
    "    old_even_invariants=[data_even_1.get_invariants()\n",
    "                         ],  # returns [:, :, 0, 0] slice which is invariants\n",
    "    old_even_covariants=[data_even_1],\n",
    "    old_odd_covariants=[data_odd_1])\n",
    "\n",
    "data_even_3, data_odd_3, invariants_even_3 = block.transform(\n",
    "    data_even_2,\n",
    "    data_odd_2,\n",
    "    data_even_1,\n",
    "    data_odd_1,\n",
    "    old_even_invariants=[data_even_1.get_invariants()\n",
    "                         ],  # returns [:, :, 0, 0] slice which is invariants\n",
    "    old_even_covariants=[data_even_1],\n",
    "    old_odd_covariants=[data_odd_1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If block contains purifiers, but fit or transform methods are called without providing necessary data it would raise ValueError.\n",
    "\n",
    "One another usefull method is get_intermediate_shapes as in StandardSequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in block.get_intermediate_shapes().items(\n",
    "):  # it is a dictionary\n",
    "    print(key, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "StandardSequence was already discussed in first tutorial \"Constructing machine learning potential\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's go to 1024 body order!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_even_now, data_odd_now = data_even_1, data_odd_1\n",
    "\n",
    "for _ in tqdm.tqdm(range(10)):\n",
    "    pca = IndividualLambdaPCAsBoth(10)\n",
    "    pca.fit(data_even_now, data_odd_now)\n",
    "    data_even_now, data_odd_now = pca.transform(data_even_now, data_odd_now)\n",
    "    expansioner = ThresholdExpansioner(50)\n",
    "    expansioner.fit(data_even_now,\n",
    "                    data_odd_now,\n",
    "                    data_even_now,\n",
    "                    data_odd_now,\n",
    "                    clebsch_gordan=clebsch)\n",
    "    data_even_now, data_odd_now = expansioner.transform(\n",
    "        data_even_now, data_odd_now, data_even_now, data_odd_now)\n",
    "\n",
    "    # very high body order cause numerical instabilities,\n",
    "    # and, thus, there is need to normalize data\n",
    "    for lambd in range(6):\n",
    "        size = data_even_now.actual_sizes_[lambd]\n",
    "        if (size > 0):\n",
    "            even_factor = np.sqrt(\n",
    "                np.mean(data_even_now.covariants_[:, :size, lambd]**2))\n",
    "            if (even_factor > 1e-15):  #catch exact zeros\n",
    "                data_even_now.covariants_[:, :size, lambd] /= even_factor\n",
    "        size = data_odd_now.actual_sizes_[lambd]\n",
    "        if (size > 0):\n",
    "            odd_factor = np.sqrt(\n",
    "                np.mean(data_odd_now.covariants_[:, :size, lambd]**2))\n",
    "            if (odd_factor > 1e-15):  #catch exact zeros\n",
    "                data_odd_now.covariants_[:, :size, lambd] /= odd_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_even_now.covariants_.shape)\n",
    "print(data_even_now.actual_sizes_)\n",
    "print(data_odd_now.actual_sizes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
