{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is not always clear how to select good hyperparameters for calculations. In the second tutorial \"Getting insights about the model\" it was shown how to plot spectrums of PCA for all lambda channels and parities. This information along with the other one, such as regression accuracy might be usefull to select better hypers. Particulary, the most straghtforward way is to select number of pca components, in a such a way to cover the most part of variance, and do it successively from block to block. \n",
    "\n",
    "In this case it is very undesirable to fit all parts of the model, including not changed ones from scratch. One possible way around is to do all things by hand, as it was described in the tutorial \"Constructor or non standard_sequence\", but there would be additional headache with packing resulting blocks into a single model with convenient .transform method. Nice toolbox has capabilities to do it very succinctly.\n",
    "\n",
    "First of all we need to get spherical expansion coefficients the same way as in previous tutorials:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-10-13 16:51:43--  https://archive.materialscloud.org/record/file?file_id=b612d8e3-58af-4374-96ba-b3551ac5d2f4&filename=methane.extxyz.gz&record_id=528\n",
      "Resolving archive.materialscloud.org (archive.materialscloud.org)... 148.187.96.41\n",
      "Connecting to archive.materialscloud.org (archive.materialscloud.org)|148.187.96.41|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 FOUND\n",
      "Location: https://object.cscs.ch/archive/b6/12/d8e3-58af-4374-96ba-b3551ac5d2f4/data?response-content-type=application%2Foctet-stream&response-content-disposition=attachment%3B%20filename%3Dmethane.extxyz.gz&Expires=1602600763&Signature=8jdCsvI6bQAAPs0Sa2XhwfTyh50%3D&AWSAccessKeyId=ee64314446074ed3ab5f375a522a4893 [following]\n",
      "--2020-10-13 16:51:43--  https://object.cscs.ch/archive/b6/12/d8e3-58af-4374-96ba-b3551ac5d2f4/data?response-content-type=application%2Foctet-stream&response-content-disposition=attachment%3B%20filename%3Dmethane.extxyz.gz&Expires=1602600763&Signature=8jdCsvI6bQAAPs0Sa2XhwfTyh50%3D&AWSAccessKeyId=ee64314446074ed3ab5f375a522a4893\n",
      "Resolving object.cscs.ch (object.cscs.ch)... 148.187.25.202, 148.187.25.200, 148.187.25.201\n",
      "Connecting to object.cscs.ch (object.cscs.ch)|148.187.25.202|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1218139661 (1.1G) [application/octet-stream]\n",
      "Saving to: ‘methane.extxyz.gz’\n",
      "\n",
      "methane.extxyz.gz   100%[===================>]   1.13G  56.1MB/s    in 27s     \n",
      "\n",
      "2020-10-13 16:52:10 (43.0 MB/s) - ‘methane.extxyz.gz’ saved [1218139661/1218139661]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 46.08it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 226.33it/s]\n"
     ]
    }
   ],
   "source": [
    "# downloading dataset from https://archive.materialscloud.org/record/2020.110\n",
    "\n",
    "!wget \"https://archive.materialscloud.org/record/file?file_id=b612d8e3-58af-4374-96ba-b3551ac5d2f4&filename=methane.extxyz.gz&record_id=528\" -O methane.extxyz.gz\n",
    "!gunzip -k methane.extxyz.gz\n",
    "\n",
    "import numpy as np\n",
    "import ase.io\n",
    "import tqdm\n",
    "from nice.blocks import *\n",
    "from nice.utilities import *\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "\n",
    "structures = ase.io.read('methane.extxyz', \n",
    "                         index = '0:1000')\n",
    "\n",
    "HYPERS = {\n",
    "'interaction_cutoff': 6.3,\n",
    "'max_radial': 5,\n",
    "'max_angular': 5,\n",
    "'gaussian_sigma_type': 'Constant',\n",
    "'gaussian_sigma_constant': 0.05,\n",
    "'cutoff_smooth_width': 0.3,\n",
    "'radial_basis': 'GTO'\n",
    "}\n",
    "\n",
    "all_species = get_all_species(structures)\n",
    "\n",
    "coefficients = get_spherical_expansion(structures, HYPERS, all_species)\n",
    "coefficients = coefficients[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "coefficients are now spherical expansion coefficients for H centered environments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000, 10, 6, 11)\n"
     ]
    }
   ],
   "source": [
    "print(coefficients.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do the first steps from standar sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "even_0, odd_0 = InitialTransformer().transform(coefficients)\n",
    "initial_pca = IndividualLambdaPCAsBoth()\n",
    "initial_pca.fit(even_0, odd_0)\n",
    "even_0_t, odd_0_t = initial_pca.transform(even_0, odd_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can fit couple of standard blocks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_1 = StandardBlock(ThresholdExpansioner(100), None, IndividualLambdaPCAsBoth(20))\n",
    "block_1.fit(even_0_t, odd_0_t, even_0_t, odd_0_t)\n",
    "even_1, odd_1, _ = block_1.transform(even_0_t, odd_0_t, even_0_t, odd_0_t)\n",
    "\n",
    "block_2 = StandardBlock(None, None, None, ThresholdExpansioner(100, mode = 'invariants'))\n",
    "block_2.fit(even_1, odd_1, even_0_t, odd_0_t)\n",
    "_, _, even_invariants = block_2.transform(even_1, odd_1, even_0_t, odd_0_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At his moment we have all parts of this standard sequence fitted:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "trans = StandardSequence(initial_pca= initial_pca, blocks = [block_1, block_2])\n",
    "print(initial_pca.is_fitted())\n",
    "print(block_1.is_fitted())\n",
    "print(block_2.is_fitted())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "what about full model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(trans.is_fitted())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nope. \n",
    "\n",
    "At this point there is very high probability to make a mistake. Particularly one can feed StandardSequence with some fitted initial_pca along with blocks, which were fitted based not on the same initial_pca, or with different initial_normalizer, or even on different data. In order to prevent it, there is requirement to pass additional flag guaranteed_parts_fitted_consistently = True to the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "trans = StandardSequence(initial_pca= initial_pca, blocks = [block_1, block_2],\n",
    "                         guaranteed_parts_fitted_consistently = True)\n",
    "print(trans.is_fitted())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model is considered to be fitted if 1) all parts are fitted and 2) if guaranteed_parts_fitted_consistently is set to be True\n",
    "\n",
    "**Golden rule:** Every time you pass guaranteed_parts_fitted_consistently = True make a pause and think twice. \n",
    "\n",
    "Let's check consistency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "even_invariants_2 = trans.transform(coefficients, return_only_invariants = True)[3]\n",
    "print(np.sum(np.abs(even_invariants - even_invariants_2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This also works in other direction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "initial_pca = IndividualLambdaPCAsBoth()\n",
    "block_1 = StandardBlock(ThresholdExpansioner(100), None, IndividualLambdaPCAsBoth(20))\n",
    "block_2 = StandardBlock(None, None, None, ThresholdExpansioner(100, mode = 'invariants'))\n",
    "\n",
    "print(initial_pca.is_fitted())\n",
    "print(block_1.is_fitted())\n",
    "print(block_2.is_fitted())\n",
    "\n",
    "trans = StandardSequence(initial_pca = initial_pca, blocks = [block_1, block_2])\n",
    "trans.fit(coefficients)\n",
    "\n",
    "print(initial_pca.is_fitted())\n",
    "print(block_1.is_fitted())\n",
    "print(block_2.is_fitted())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "StandardBlock behaves the same way:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "expansioner, pca = ThresholdExpansioner(100), IndividualLambdaPCAsBoth(20)\n",
    "print(expansioner.is_fitted())\n",
    "print(pca.is_fitted())\n",
    "\n",
    "block = StandardBlock(expansioner, None, pca)\n",
    "block.fit(even_0_t, odd_0_t, even_0_t, odd_0_t)\n",
    "\n",
    "print(expansioner.is_fitted())\n",
    "print(pca.is_fitted())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "expansioner, pca = ThresholdExpansioner(100), IndividualLambdaPCAsBoth(20)\n",
    "expansioner.fit(even_0_t, odd_0_t, even_0_t, odd_0_t)\n",
    "even_1, odd_1 = expansioner.transform(even_0_t, odd_0_t, even_0_t, odd_0_t)\n",
    "pca.fit(even_1, odd_1)\n",
    "\n",
    "block = StandardBlock(expansioner, None, pca, \n",
    "                      guaranteed_parts_fitted_consistently = True)\n",
    "\n",
    "print(block.is_fitted())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is another group of blocks, which accepts classes such as sklearn.linear_model.Ridge in the initialization. But in their case there is need to apply several distinct regressors, separatelly for each lambda channel and parity. Thus, input regressor is clonned, and initial instances is not touched in any way. So, the material of this tutorials does not apply to purifiers. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
