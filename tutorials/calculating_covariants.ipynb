{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating covariants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous tutorial, we calculated invariant representations of atomic environments and used them for the prediction of energies - invariant properties. \n",
    "\n",
    "In the case when there is a need to predict covariant properties, covariants instead of invariants are required. This tutorial shows how to calculate them.\n",
    "\n",
    "First of all, we need to get **fitted** instance of the model as in the previous tutorial. It is done by the following preliminaries cell: (with the only difference that since we want to calculate covariants, we clearly shouldn't leave the covariants branch of the last block empty) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell to wrap in collapsible in future\n",
    "\n",
    "# downloading dataset from https://archive.materialscloud.org/record/2020.110\n",
    "\n",
    "!wget \"https://archive.materialscloud.org/record/file?file_id=b612d8e3-58af-4374-96ba-b3551ac5d2f4&filename=methane.extxyz.gz&record_id=528\" -O methane.extxyz.gz\n",
    "!gunzip -k methane.extxyz.gz\n",
    "\n",
    "import numpy as np\n",
    "import ase.io\n",
    "import tqdm\n",
    "from nice.blocks import *\n",
    "from nice.utilities import *\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "\n",
    "HARTREE_TO_EV = 27.211386245988\n",
    "train_subset = \"0:10000\"  #input for ase.io.read command\n",
    "test_subset = \"10000:15000\"  #input to ase.io.read command\n",
    "environments_for_fitting = 1000  #number of environments to fit nice transfomers\n",
    "grid = [150, 200, 350, 500, 750, 1000, 1500, 2000, 3000, 5000, 7500,\n",
    "        10000]  #for learning curve\n",
    "\n",
    "#HYPERS for librascal spherical expansion coefficients\n",
    "HYPERS = {\n",
    "    'interaction_cutoff': 6.3,\n",
    "    'max_radial': 5,\n",
    "    'max_angular': 5,\n",
    "    'gaussian_sigma_type': 'Constant',\n",
    "    'gaussian_sigma_constant': 0.05,\n",
    "    'cutoff_smooth_width': 0.3,\n",
    "    'radial_basis': 'GTO'\n",
    "}\n",
    "\n",
    "\n",
    "#our model:\n",
    "def get_nice():\n",
    "    return StandardSequence([\n",
    "        StandardBlock(ThresholdExpansioner(num_expand=150),\n",
    "                      CovariantsPurifierBoth(max_take=10),\n",
    "                      IndividualLambdaPCAsBoth(n_components=50),\n",
    "                      ThresholdExpansioner(num_expand=300, mode='invariants'),\n",
    "                      InvariantsPurifier(max_take=50),\n",
    "                      InvariantsPCA(n_components=200)),\n",
    "        StandardBlock(ThresholdExpansioner(num_expand=150),\n",
    "                      CovariantsPurifierBoth(max_take=10),\n",
    "                      IndividualLambdaPCAsBoth(n_components=10),\n",
    "                      ThresholdExpansioner(num_expand=300, mode='invariants'),\n",
    "                      InvariantsPurifier(max_take=50),\n",
    "                      InvariantsPCA(n_components=200)),\n",
    "        StandardBlock(ThresholdExpansioner(num_expand=150),\n",
    "                      CovariantsPurifierBoth(max_take=10), None,\n",
    "                      ThresholdExpansioner(num_expand=300, mode='invariants'),\n",
    "                      InvariantsPurifier(max_take=50),\n",
    "                      InvariantsPCA(n_components=200))\n",
    "    ],\n",
    "                            initial_scaler=InitialScaler(\n",
    "                                mode='signal integral', individually=True))\n",
    "\n",
    "\n",
    "train_structures = ase.io.read('methane.extxyz', index=train_subset)\n",
    "\n",
    "test_structures = ase.io.read('methane.extxyz', index=test_subset)\n",
    "\n",
    "all_species = get_all_species(train_structures + test_structures)\n",
    "\n",
    "train_coefficients = get_spherical_expansion(train_structures, HYPERS,\n",
    "                                             all_species)\n",
    "\n",
    "test_coefficients = get_spherical_expansion(test_structures, HYPERS,\n",
    "                                            all_species)\n",
    "\n",
    "#individual nice transformers for each atomic specie in the dataset\n",
    "nice = {}\n",
    "for key in train_coefficients.keys():\n",
    "    nice[key] = get_nice()\n",
    "\n",
    "for key in train_coefficients.keys():\n",
    "    nice[key].fit(train_coefficients[key][:environments_for_fitting])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to call **.transform** method with **return_only_invariants = False**, which is the default value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_even, data_odd, invariants_even = nice[1].transform(train_coefficients[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result is **data_even**, **data_odd** and **invariants_even**. The first two objects are covariants. The last one is invariants. \n",
    "\n",
    "There is another important symmetry in addition to the translational and rotational one. Usually, atomic properties, such as energy, also transform in a certain way with respect to inversion. Particularly, energy is invariant with respect to it. \n",
    "\n",
    "In NICE, features are separated into two groups - the ones which are invariant with respect to inversion and the ones that change their sign. The first ones are called even; the second ones are called odd. \n",
    "\n",
    "Now let's take a look at the returned objects more closely:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Invariants** is the same object as in the previous tutorial - dictionary, where keys are body order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in invariants_even.keys():\n",
    "    print(invariants_even[key].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returned covariants are covariants after the last block, i. e. in our case of body order 4. \n",
    "(functionality to get all covariants of all body order from **StandardSequence** will be added in the next version of NICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even covariants are packed in the class Data, which has two relevant fields - \n",
    "**.covariants_** and **.actual_sizes_**. (getters are also to be added in the next version) First is np.array with covariants themselves. It has following indexing -**[environmental_index, feature_index, lambda, m]**. But the problem is that for each lambda channel, the actual number of features is different. Thus, the shape of this array doesn't reflect the real number of meaningful entries. Information about the actual number of features is stored in **.actual_sizes_**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(data_even))\n",
    "print(\"shape of even covariants array: {}\".format(data_even.covariants_.shape))\n",
    "print(\"actual sizes of even covariants: {}\".format(data_even.actual_sizes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is the same for odd covariants:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"shape of odd covariants array: {}\".format(data_odd.covariants_.shape))\n",
    "print(\"actual sizes of odd covariants: {}\".format(data_odd.actual_sizes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is one other point - for each lambda channel the size of covariant vectors is (2 * lambda + 1). These vectors are stored from the beginning. It means that the meaningful entries for each lambda are located in **[:, :, lambda, :(2 * lambda + 1)]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the [nice article](https://aip.scitation.org/doi/10.1063/5.0021116) another definition of **parity** is used. Covariants are split into **true** and **pseudo** groups. All the covariants in the **true** group are transformed with respect to inversion as (-1)^lambda, while all the covariants in the **pseudo** group are transformed as (-1) ^ (lambda + 1). \n",
    "\n",
    "There is a special class - **ParityDefinitionChanger** to switch between these definitions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_true, data_pseudo = ParityDefinitionChanger().transform(\n",
    "    data_even, data_odd)\n",
    "\n",
    "print(data_true.covariants_.shape)\n",
    "print(data_true.actual_sizes_)\n",
    "\n",
    "print(data_pseudo.covariants_.shape)\n",
    "print(data_pseudo.actual_sizes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this transformation is symmetric, we can use this once again to go back from the true and pseudo covariants to even and odd:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_even, data_odd = ParityDefinitionChanger().transform(\n",
    "    data_true, data_pseudo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is one other discrepancy - covariants defined in the nice article, are smaller by the factor of (2 * lambda + 1). Thus, the last step to get full compliance is the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lambd in range(6):\n",
    "    data_true.covariants_[:, :data_true.actual_sizes_[lambd],\n",
    "                          lambd, :(2 * lambd + 1)] /= (2 * lambd + 1)\n",
    "    data_pseudo.covariants_[:, :data_pseudo.actual_sizes_[lambd],\n",
    "                            lambd, :(2 * lambd + 1)] /= (2 * lambd + 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
