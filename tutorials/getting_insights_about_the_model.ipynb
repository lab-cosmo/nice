{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting insights about the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first tutorial, we calculated invariant representations of atomic environments and used them for prediction of energies.\n",
    "\n",
    "But it is always good to have some understanding about the model. In this tutorial, it will be shown how to get spectrums of pca along with the number of covariants after each transformation.\n",
    "\n",
    "First of all, we need **fitted** model. This preliminary cell reproduces corresponding part of the first tutorial \"constructing machine learning potential\": (few hypers are changed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell to wrap in collapsible in future\n",
    "\n",
    "# downloading dataset from https://archive.materialscloud.org/record/2020.110\n",
    "\n",
    "!wget \"https://archive.materialscloud.org/record/file?file_id=b612d8e3-58af-4374-96ba-b3551ac5d2f4&filename=methane.extxyz.gz&record_id=528\" -O methane.extxyz.gz\n",
    "!gunzip -k methane.extxyz.gz\n",
    "\n",
    "import numpy as np\n",
    "import ase.io\n",
    "import tqdm\n",
    "from nice.blocks import *\n",
    "from nice.utilities import *\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "\n",
    "HARTREE_TO_EV = 27.211386245988\n",
    "train_subset = \"0:10000\"  #input for ase.io.read command\n",
    "test_subset = \"10000:15000\"  #input to ase.io.read command\n",
    "environments_for_fitting = 1000  #number of environments to fit nice transfomers\n",
    "grid = [150, 200, 350, 500, 750, 1000, 1500, 2000, 3000, 5000, 7500,\n",
    "        10000]  #for learning curve\n",
    "\n",
    "#HYPERS for librascal spherical expansion coefficients\n",
    "HYPERS = {\n",
    "    'interaction_cutoff': 6.3,\n",
    "    'max_radial': 5,\n",
    "    'max_angular': 5,\n",
    "    'gaussian_sigma_type': 'Constant',\n",
    "    'gaussian_sigma_constant': 0.05,\n",
    "    'cutoff_smooth_width': 0.3,\n",
    "    'radial_basis': 'GTO'\n",
    "}\n",
    "\n",
    "\n",
    "#our model:\n",
    "def get_nice():\n",
    "    return StandardSequence([\n",
    "        StandardBlock(ThresholdExpansioner(num_expand=150),\n",
    "                      CovariantsPurifierBoth(max_take=10),\n",
    "                      IndividualLambdaPCAsBoth(n_components=50),\n",
    "                      ThresholdExpansioner(num_expand=300, mode='invariants'),\n",
    "                      InvariantsPurifier(max_take=50),\n",
    "                      InvariantsPCA(n_components=200)),\n",
    "        StandardBlock(ThresholdExpansioner(num_expand=150),\n",
    "                      CovariantsPurifierBoth(max_take=10),\n",
    "                      IndividualLambdaPCAsBoth(n_components=50),\n",
    "                      ThresholdExpansioner(num_expand=300, mode='invariants'),\n",
    "                      InvariantsPurifier(max_take=50),\n",
    "                      InvariantsPCA(n_components=200)),\n",
    "        StandardBlock(ThresholdExpansioner(num_expand=150),\n",
    "                      CovariantsPurifierBoth(max_take=10),\n",
    "                      IndividualLambdaPCAsBoth(n_components=50),\n",
    "                      ThresholdExpansioner(num_expand=300, mode='invariants'),\n",
    "                      InvariantsPurifier(max_take=50),\n",
    "                      InvariantsPCA(n_components=200))\n",
    "    ],\n",
    "                            initial_scaler=InitialScaler(\n",
    "                                mode='signal integral', individually=True))\n",
    "\n",
    "\n",
    "train_structures = ase.io.read('methane.extxyz', index=train_subset)\n",
    "\n",
    "test_structures = ase.io.read('methane.extxyz', index=test_subset)\n",
    "\n",
    "all_species = get_all_species(train_structures + test_structures)\n",
    "\n",
    "train_coefficients = get_spherical_expansion(train_structures, HYPERS,\n",
    "                                             all_species)\n",
    "\n",
    "test_coefficients = get_spherical_expansion(test_structures, HYPERS,\n",
    "                                            all_species)\n",
    "\n",
    "#individual nice transformers for each atomic specie in the dataset\n",
    "nice = {}\n",
    "for key in train_coefficients.keys():\n",
    "    nice[key] = get_nice()\n",
    "\n",
    "for key in train_coefficients.keys():\n",
    "    nice[key].fit(train_coefficients[key][:environments_for_fitting])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it was discussed in the first tutorial, **ThresholdExpansioner** sorts all pairs of inputs by their pairwise importances and after that produces the output only for fixed number of most important pairs. This number is controlled by **num_expand**. \n",
    "\n",
    "However, there are two reasons, why the real number of covariants after **ThresholdEpansioner** might be different from the specified one. \n",
    "1) Some pairs of input covariants do not produce features to all lambda channels. Particularly, pair of input covariants with some l1 and l2 produces covariants only to lambda channels where |l1 - l2| <= lambda <= l1 + l2. Thus, real number of features after **ThresholdExpanioner** would be smaller than specified one in **num_expand**.\n",
    "\n",
    "2) Pairwise importances can have a lot of collisions. For instance, it is impossible to select such a threshold to filter out exactly 3 pairs from the set of pairs with the following importances [1, 1, 2, 2]. It is possible to filter out either 0, either 2, either 4, but not exactly 3. \n",
    "\n",
    "Thus, it is a good idea to have possibility to look at the actual amount of intermediate features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**StandardSequence** has a method **get_intermediat_shapes()**. It returns intermediate shapes in the form of nested dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediate_shapes = nice[1].get_intermediate_shapes()\n",
    "\n",
    "for key in intermediate_shapes.keys():\n",
    "    print(key, ':', intermediate_shapes[key], end='\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spectrums of pcas can be accessed in the following way: \n",
    "(convenient getters will be inserted in the next version of NICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proper_log_plot(array, *args, **kwargs):\n",
    "    '''avoiding log(0)'''\n",
    "    plt.plot(np.arange(len(array)) + 1, array, *args, **kwargs)\n",
    "    plt.ylim([1e-3, 1e0])\n",
    "\n",
    "\n",
    "colors = ['r', 'g', 'b', 'orange', 'yellow', 'purple']\n",
    "\n",
    "print(\"nu: \", 1)\n",
    "for i in range(6):  # loop over lambda channels\n",
    "    if (nice[6].initial_pca_ is not None):\n",
    "        if (nice[6].initial_pca_.even_pca_.pcas_[i] is not None):\n",
    "            proper_log_plot(\n",
    "                nice[6].initial_pca_.even_pca_.pcas_[i].importances_,\n",
    "                color=colors[i],\n",
    "                label=\"lambda = {}\".format(i))\n",
    "\n",
    "for i in range(6):  # loop over lambda channels\n",
    "    if (nice[6].initial_pca_ is not None):\n",
    "        if (nice[6].initial_pca_.odd_pca_.pcas_[i] is not None):\n",
    "            proper_log_plot(\n",
    "                nice[6].initial_pca_.odd_pca_.pcas_[i].importances_,\n",
    "                '--',\n",
    "                color=colors[i],\n",
    "                label=\"lambda = {}\".format(i))\n",
    "\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "for nu in range(len(nice[6].blocks_)):  # loop over body orders\n",
    "    print(\"nu: \", nu + 2)\n",
    "    for i in range(6):  # loop over lambda channels\n",
    "        if (nice[6].blocks_[nu].covariants_pca_ is not None):\n",
    "            if (nice[6].blocks_[nu].covariants_pca_.even_pca_.pcas_[i]\n",
    "                    is not None):\n",
    "                proper_log_plot(nice[6].blocks_[nu].covariants_pca_.even_pca_.\n",
    "                                pcas_[i].importances_,\n",
    "                                color=colors[i],\n",
    "                                label=\"lambda = {}\".format(i))\n",
    "\n",
    "    for i in range(6):  # loop over lambda channels\n",
    "        if (nice[6].blocks_[nu].covariants_pca_ is not None):\n",
    "            if (nice[6].blocks_[nu].covariants_pca_.odd_pca_.pcas_[i]\n",
    "                    is not None):\n",
    "                proper_log_plot(nice[6].blocks_[nu].covariants_pca_.odd_pca_.\n",
    "                                pcas_[i].importances_,\n",
    "                                '--',\n",
    "                                color=colors[i])\n",
    "\n",
    "    plt.yscale('log')\n",
    "    plt.xscale('log')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(checks if pca instance is **None** are needed since it would be **None** if the number of features for corresponding lambda channel would be zero after the expansion step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inner class for single Lambda channel inherits from sklearn.decomposition.TruncatedSVD (PCA without centering the data, which would break covariant transformation). Thus, in addition to **.importances_**, **.explained_variance_** and **.explained_variance_ratio_** are also accesible. \n",
    "\n",
    "**importances_** (which are used by subsequent **TresholdExpansioners**) are **explained_variance_** normalized not to variance of input as **explained_variance_ratio_**, but to variance of output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sum(nice[6].blocks_[1].\\\n",
    "             covariants_pca_.even_pca_.pcas_[2].explained_variance_))\n",
    "print(np.sum(nice[6].blocks_[1].\\\n",
    "             covariants_pca_.even_pca_.pcas_[2].explained_variance_ratio_))\n",
    "print(np.sum(nice[6].blocks_[1].\\\n",
    "             covariants_pca_.even_pca_.pcas_[2].importances_))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
